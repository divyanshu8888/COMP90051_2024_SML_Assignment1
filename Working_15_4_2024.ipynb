{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the datasets\n",
        "df1 = pd.read_json(\"domain1_train_data.json\", lines=True)\n",
        "df2 = pd.read_json(\"domain2_train_data.json\", lines=True)\n",
        "\n",
        "# Function to prepare text data from token IDs to a space-separated string\n",
        "def prepare_text(data):\n",
        "    data['text'] = data['text'].apply(lambda x: ' '.join(map(str, x)))\n",
        "    return data\n",
        "\n",
        "# Prepare the text data\n",
        "df1 = prepare_text(df1)\n",
        "df2 = prepare_text(df2)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
        "\n",
        "# Vectorize the combined text data\n",
        "X = vectorizer.fit_transform(combined_df['text'])\n",
        "y = combined_df['label']\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_pred = svm_model.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "svm_conf_matrix = confusion_matrix(y_test, svm_pred)\n",
        "svm_class_report = classification_report(y_test, svm_pred)\n",
        "\n",
        "print(\"Support Vector Machine (SVM) Results:\")\n",
        "print(f\"Accuracy: {svm_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(svm_conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(svm_class_report)\n",
        "\n",
        "# 1D Convolutional Neural Network (CNN)\n",
        "# Convert text data to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(combined_df['text'])\n",
        "X_seq = tokenizer.texts_to_sequences(combined_df['text'])\n",
        "max_sequence_length = max([len(seq) for seq in X_seq])\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
        "    X_pad, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
        "    Conv1D(64, 3, activation='relu'),\n",
        "    MaxPooling1D(4),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train_seq, y_train_seq, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "cnn_pred = (cnn_model.predict(X_test_seq) > 0.5).astype(\"int32\")\n",
        "cnn_accuracy = accuracy_score(y_test_seq, cnn_pred)\n",
        "cnn_conf_matrix = confusion_matrix(y_test_seq, cnn_pred)\n",
        "cnn_class_report = classification_report(y_test_seq, cnn_pred)\n",
        "\n",
        "print(\"\\n1D Convolutional Neural Network (CNN) Results:\")\n",
        "print(f\"Accuracy: {cnn_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cnn_conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(cnn_class_report)\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
        "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_seq, y_train_seq, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "lstm_pred = (lstm_model.predict(X_test_seq) > 0.5).astype(\"int32\")\n",
        "lstm_accuracy = accuracy_score(y_test_seq, lstm_pred)\n",
        "lstm_conf_matrix = confusion_matrix(y_test_seq, lstm_pred)\n",
        "lstm_class_report = classification_report(y_test_seq, lstm_pred)\n",
        "\n",
        "print(\"\\nLong Short-Term Memory (LSTM) Results:\")\n",
        "print(f\"Accuracy: {lstm_accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(lstm_conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(lstm_class_report)\n"
      ],
      "metadata": {
        "id": "lTMZd6TO1bYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I got the accuracy of 72% with this, i want to increase it further\n",
        "\n",
        "Training:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import shap\n",
        "\n",
        "# Load your datasets\n",
        "df1 = pd.read_json(\"/content/domain1_train_data.json\", lines=True)\n",
        "df2 = pd.read_json(\"/content/domain2_train_data.json\", lines=True)\n",
        "\n",
        "# Function to prepare text data from token IDs to a space-separated string\n",
        "def prepare_text(data):\n",
        "    data['text'] = data['text'].apply(lambda x: ' '.join(map(str, x)))\n",
        "    return data\n",
        "\n",
        "# Apply text preparation\n",
        "df1 = prepare_text(df1)\n",
        "df2 = prepare_text(df2)\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Vectorize text data with TfidfVectorizer instead of CountVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)  # limit to 10000 most important features\n",
        "X = vectorizer.fit_transform(combined_df['text'])\n",
        "y = combined_df['label']\n",
        "\n",
        "# Handling class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Split data into training and testing sets using stratified split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Setup a Logistic Regression model within a GridSearchCV to tune hyperparameters\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'saga']  # 'saga' works well with large datasets and supports l1 penalty\n",
        "}\n",
        "model = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=StratifiedKFold(5), scoring='accuracy')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {model.best_params_}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Model Explainability with SHAP\n",
        "explainer = shap.LinearExplainer(model.best_estimator_, X_train, feature_perturbation=\"interventional\")\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, feature_names=vectorizer.get_feature_names_out())\n",
        "\n",
        "#Testing prediction code:\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Assuming the earlier defined functions and model training code is available and executed\n",
        "# ... [All the earlier code for model training]\n",
        "\n",
        "# Load the test dataset\n",
        "df_test = pd.read_json(\"/content/test_data.json\", lines=True)\n",
        "\n",
        "# Prepare the test text data from token IDs to a space-separated string\n",
        "df_test['text'] = df_test['text'].apply(lambda x: ' '.join(map(str, x)))\n",
        "\n",
        "# Transform the test data using the fitted TfidfVectorizer from the training\n",
        "X_test = vectorizer.transform(df_test['text'])\n",
        "\n",
        "# Predict on the test data using the trained model\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Store the predictions in a dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'predicted_label': test_predictions\n",
        "})\n",
        "\n",
        "# Export the predictions to a CSV file\n",
        "results_df.to_csv('test_predictions.csv', index=False)\n",
        "\n",
        "# Since we are working in a simulated Python environment,\n",
        "# the file will be saved in the virtual storage provided.\n",
        "# You would need to adjust the file path according to your environment.\n"
      ],
      "metadata": {
        "id": "i8aXwCcKJ_ML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}