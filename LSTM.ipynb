{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6c1rBbXFtWj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-dcaOhrGXRC"
      },
      "outputs": [],
      "source": [
        "# Custom metrics definitions\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "sHUn_0mWSrJh",
        "outputId": "1e73dc8f-a399-4644-8901-50279a799779"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unexpected character found when decoding array value (1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dab653e8e4be>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/test_data.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain1_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain2_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m                         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                         \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m                         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding array value (1)"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "domain1_path = \"/content/domain1_train_data.json\"\n",
        "domain2_path = \"/content/domain2_train_data.json\"\n",
        "test_data_path = \"/content/test_data.json\"\n",
        "\n",
        "df1 = pd.read_json(domain1_path, lines=True)\n",
        "df2 = pd.read_json(domain2_path, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Ke4UDWGf2V"
      },
      "outputs": [],
      "source": [
        "# Prepare data function\n",
        "MAX_LENGTH = 1200\n",
        "def prepare_data(df):\n",
        "    texts = [[token + 1 for token in text] for text in df['text']]\n",
        "    X_pad = pad_sequences(texts, maxlen=MAX_LENGTH)\n",
        "    y = np.array(df['label'])\n",
        "    return X_pad, y\n",
        "\n",
        "X_balanced, y_balanced = prepare_data(df1)\n",
        "X_imbalanced, y_imbalanced = prepare_data(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qC1r9XDpLoE",
        "outputId": "63170701-5b8c-4cc4-b6c4-2fd3992c942f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "250/250 [==============================] - 36s 135ms/step - loss: 0.6688 - accuracy: 0.6240 - f1_m: 0.7601 - precision_m: 0.6259 - recall_m: 0.9909 - val_loss: 0.9582 - val_accuracy: 0.0000e+00 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 23s 92ms/step - loss: 0.5983 - accuracy: 0.6913 - f1_m: 0.7803 - precision_m: 0.6955 - recall_m: 0.9211 - val_loss: 0.8659 - val_accuracy: 0.4640 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 19s 75ms/step - loss: 0.3765 - accuracy: 0.8545 - f1_m: 0.8818 - precision_m: 0.8608 - recall_m: 0.9161 - val_loss: 0.6597 - val_accuracy: 0.6510 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 0.2125 - accuracy: 0.9273 - f1_m: 0.9381 - precision_m: 0.9280 - recall_m: 0.9565 - val_loss: 1.2043 - val_accuracy: 0.5280 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 1/20\n",
            "650/650 [==============================] - 73s 113ms/step - loss: 0.4035 - accuracy: 0.8510 - f1_m: 0.0929 - precision_m: 0.1395 - recall_m: 0.0807 - val_loss: 0.1698 - val_accuracy: 0.9585 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/20\n",
            "650/650 [==============================] - 53s 82ms/step - loss: 0.2052 - accuracy: 0.9296 - f1_m: 0.6276 - precision_m: 0.6896 - recall_m: 0.6227 - val_loss: 0.0688 - val_accuracy: 0.9773 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 3/20\n",
            "650/650 [==============================] - 45s 69ms/step - loss: 0.0778 - accuracy: 0.9787 - f1_m: 0.8297 - precision_m: 0.8478 - recall_m: 0.8347 - val_loss: 0.0974 - val_accuracy: 0.9638 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# Model definition\n",
        "EMBEDDING_DIM = 50\n",
        "VOCAB_SIZE = max(np.max(X_balanced), np.max(X_imbalanced)) + 1\n",
        "\n",
        "def create_lstm_model(vocab_size, embedding_dim, max_length):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        LSTM(16, return_sequences=True),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train on the balanced dataset\n",
        "model = create_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "model_checkpoint_balanced = ModelCheckpoint('/content/model_balanced_weights.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "model.fit(X_balanced, y_balanced, batch_size=16, epochs=20, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=1), model_checkpoint_balanced])\n",
        "\n",
        "# # Prepare the SMOTE-balanced version of the imbalanced dataset\n",
        "# smote = SMOTE()\n",
        "# X_smote, y_smote = smote.fit_resample(X_imbalanced, y_imbalanced)\n",
        "\n",
        "# Continue training on the SMOTE-balanced imbalanced dataset\n",
        "model.load_weights('/content/model_balanced_weights.h5')  # Load the best weights from the first training phase\n",
        "model_checkpoint_imbalanced = ModelCheckpoint('/content/model_smote_imbalanced_weights.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "model.fit(X_imbalanced, y_imbalanced, batch_size=16, epochs=20, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=1), model_checkpoint_imbalanced])\n",
        "\n",
        "# Optionally: Evaluate the final model performance on a separate test set or validation set\n",
        "\n",
        "final_model_weights_path = '/content/final_model_weights.h5'\n",
        "model.save_weights(final_model_weights_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYcXDKSnriZw"
      },
      "outputs": [],
      "source": [
        "final_model_weights_path = '/content/final_model_weights.h5'  # Update this path as necessary\n",
        "\n",
        "# Assuming create_lstm_model is defined as shown previously\n",
        "final_model = create_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH)\n",
        "\n",
        "# Load the final model weights\n",
        "final_model.load_weights(final_model_weights_path)\n",
        "\n",
        "# Compile the model if you plan on using evaluation metrics or training further\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "X_test = pd.read_json(\"/content/test_data.json\", lines=True)\n",
        "\n",
        "# Assuming the preprocessing steps remain consistent with training\n",
        "texts_test = [[token + 1 for token in text] for text in X_test['text']]\n",
        "X_test_pad = pad_sequences(texts_test, maxlen=MAX_LENGTH)\n",
        "\n",
        "# Make predictions with the final model\n",
        "predictions = final_model.predict(X_test_pad).flatten()\n",
        "\n",
        "# Convert predictions to binary labels\n",
        "y_pred_final = (predictions > 0.5).astype(int)\n",
        "\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'class': y_pred_final})\n",
        "\n",
        "# Display counts of predicted classes\n",
        "print(Counter(y_pred_final))\n",
        "\n",
        "# Save to CSV for submission\n",
        "submission_file_path = '/content/kaggle_submission.csv'\n",
        "submission_df.to_csv(submission_file_path, index=False)\n",
        "\n",
        "print(f\"Submission saved to {submission_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ka8mu9aqLKr4",
        "outputId": "200f404b-d738-40f1-e713-8c8f09848be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution in df1:\n",
            "label\n",
            "1    2500\n",
            "0    2500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution in df2:\n",
            "label\n",
            "0    11500\n",
            "1     1500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# For df1\n",
        "label_counts_df1 = df1['label'].value_counts()\n",
        "print(\"Label distribution in df1:\")\n",
        "print(label_counts_df1)\n",
        "\n",
        "# For df2\n",
        "label_counts_df2 = df2['label'].value_counts()\n",
        "print(\"\\nLabel distribution in df2:\")\n",
        "print(label_counts_df2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8hiKPT0FkNDu",
        "outputId": "550d9fd2-8dd0-4eba-cae4-6d9b7e24a4de"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'texts' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b5b8938502be>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Preparing the balanced dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_balanced_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
          ]
        }
      ],
      "source": [
        "# Assume X_balanced_pad, y_balanced, create_lstm_model(), etc., are already defined as before\n",
        "\n",
        "# Preparing the balanced dataset\n",
        "X_balanced_pad = pad_sequences(texts, maxlen=MAX_LENGTH)\n",
        "y_balanced = np.array(labels)\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=1)  # Adjusted patience\n",
        "checkpoint = ModelCheckpoint(model_weights_file, monitor=\"val_loss\", mode=\"min\", save_weights_only=True, save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "# Splitting the dataset\n",
        "X_balanced_train, X_balanced_val, y_balanced_train, y_balanced_val = train_test_split(\n",
        "    X_balanced_pad, y_balanced, test_size=0.2, stratify=y_balanced\n",
        ")\n",
        "\n",
        "# Creating the model\n",
        "model = create_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, MAX_LENGTH)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "# Training the model on the balanced dataset\n",
        "history_balanced = model.fit(X_balanced_train, y_balanced_train,\n",
        "                             epochs=20,\n",
        "                             batch_size=64,\n",
        "                             validation_data=(X_balanced_val, y_balanced_val),\n",
        "                             callbacks=[early_stop, checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdtnPHqcknuw"
      },
      "outputs": [],
      "source": [
        "# Applying SMOTE to the imbalanced dataset\n",
        "X_imbalanced_pad = pad_sequences(texts_im, maxlen=MAX_LENGTH)\n",
        "y_imbalanced = np.array(labels_im)\n",
        "smote = SMOTE()\n",
        "X_smote, y_smote = smote.fit_resample(X_imbalanced_pad, y_imbalanced)\n",
        "\n",
        "# Splitting the SMOTE-balanced dataset\n",
        "X_smote_train, X_smote_val, y_smote_train, y_smote_val = train_test_split(\n",
        "    X_smote, y_smote, test_size=0.2, stratify=y_smote\n",
        ")\n",
        "\n",
        "# Continue training on the SMOTE-balanced dataset\n",
        "history_imbalanced_continued = model.fit(X_smote_train, y_smote_train,\n",
        "                                         epochs=20,\n",
        "                                         batch_size=64,\n",
        "                                         validation_data=(X_smote_val, y_smote_val),\n",
        "                                         callbacks=[early_stop, checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c19oEGCHYrG_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming MAX_LENGTH and model are defined somewhere above this snippet\n",
        "# MAX_LENGTH = ...\n",
        "# model = ...\n",
        "\n",
        "# Load the test set\n",
        "X_test = pd.read_json(\"/content/test_data.json\", lines=True)\n",
        "\n",
        "# Preprocess test data\n",
        "texts_test = [[token + 1 for token in text] for text in X_test['text']]\n",
        "X_test_pad = pad_sequences(texts_test, maxlen=MAX_LENGTH)\n",
        "\n",
        "# Make predictions with the model\n",
        "predictions = model.predict(X_test_pad).flatten()\n",
        "\n",
        "# Convert predictions to binary labels\n",
        "y_pred_final = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Prepare submission DataFrame\n",
        "submission_df = pd.DataFrame({'id': X_test['id'], 'class': y_pred_final})\n",
        "\n",
        "# Display counts of predicted classes\n",
        "print(Counter(y_pred_final))\n",
        "\n",
        "# Save to CSV for submission\n",
        "submission_file_path = '/content/kaggle_submission.csv'\n",
        "submission_df.to_csv(submission_file_path, index=False)\n",
        "print(f\"Submission saved to {submission_file_path}\")\n",
        "\n",
        "# Display the result DataFrame\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}